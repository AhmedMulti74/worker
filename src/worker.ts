// src/worker.ts

import { createClient, SupabaseClient, RealtimeChannel } from '@supabase/supabase-js';
import dotenv from 'dotenv';
import { scrapePage } from './scraper.js';
import { extractText } from './parse.js';
import { analyzeTextWithAI } from './extract_plans.js';
import type { ScrapeSessionPayload, Competitor, ExtractedPlan } from './types.js'; // Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ù…Ù„Ù Ù…Ø±ÙƒØ²ÙŠ

// ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…Ù† Ù…Ù„Ù .env
dotenv.config();

// Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
if (!process.env.SUPABASE_URL || !process.env.SUPABASE_SERVICE_ROLE_KEY) {
    console.error('âŒ FATAL: Supabase environment variables not found in .env file');
    process.exit(1);
}

// ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ Supabase Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØªØ§Ø­ Ø§Ù„Ø®Ø¯Ù…Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙ„Ø§Ø­ÙŠØ§Øª ÙƒØ§Ù…Ù„Ø©
const supabase: SupabaseClient = createClient(
    process.env.SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY
);

/**
 * ÙŠØ¶Ø¹ Ø¹Ù„Ø§Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø£Ù†Ù‡Ø§ ØºÙŠØ± Ø­Ø§Ù„ÙŠØ© (Ø£Ø±Ø´ÙØªÙ‡Ø§) Ù‚Ø¨Ù„ Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.
 * @param {string} competitorId - Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³
 */
async function archiveOldData(competitorId: string): Promise<void> {
    console.log(`Archiving old data for competitor ID: ${competitorId}`);
    
    const { data: old_plans, error: fetchError } = await supabase
        .from('pricing_plans')
        .select('id')
        .eq('competitor_id', competitorId)
        .eq('is_current', true);

    if (fetchError) {
        console.error('Error fetching old plans to archive:', fetchError.message);
        return;
    }
    
    if (!old_plans || old_plans.length === 0) {
        console.log('No current data to archive.');
        return;
    }

    const oldPlanIds = old_plans.map(p => p.id);

    await supabase.from('plan_features').update({ is_current: false }).in('plan_id', oldPlanIds);
    await supabase.from('pricing_plans').update({ is_current: false }).in('id', oldPlanIds);
    
    console.log(`Archived ${oldPlanIds.length} old plans and their features.`);
}

/**
 * Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù„Ø³Ø© ÙƒØ´Ø· Ø¬Ø¯ÙŠØ¯Ø©
 * @param {ScrapeSessionPayload} session - Ø³Ø¬Ù„ Ø¬Ù„Ø³Ø© Ø§Ù„ÙƒØ´Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
 */
async function processScrapeSession(session: ScrapeSessionPayload): Promise<void> {
    const { data: competitor, error: competitorError } = await supabase
        .from('competitors')
        .select('id, name, pricing_page_url')
        .eq('id', session.competitor_id)
        .single<Competitor>(); // ØªØ­Ø¯ÙŠØ¯ Ø£Ù†Ù†Ø§ Ù†ØªÙˆÙ‚Ø¹ ÙƒØ§Ø¦Ù† Competitor ÙˆØ§Ø­Ø¯

    if (competitorError || !competitor) {
        const errorMessage = `Competitor not found for session ID: ${session.id}. Error: ${competitorError?.message}`;
        console.error(errorMessage);
        await supabase.from('scrape_sessions').update({ status: 'failed', error_message: errorMessage }).eq('id', session.id);
        return;
    }

    console.log(`\nğŸš€ New job received for competitor: ${competitor.name} (Session ID: ${session.id})`);
    
    try {
        // --- ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø«Ù„Ø§Ø« Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ ---
        await scrapePage(competitor.pricing_page_url);
        const textContent = await extractText();
        const plans: ExtractedPlan[] = await analyzeTextWithAI(textContent);
        
        if (plans.length === 0) {
            throw new Error("AI model did not return any plans in the expected format.");
        }

        await archiveOldData(competitor.id);

        for (const plan of plans) {
            const { data: planData, error: planError } = await supabase
                .from('pricing_plans')
                .insert({
                    scrape_session_id: session.id,
                    competitor_id: competitor.id,
                    plan_name: plan.planName,
                    price: typeof plan.price === 'number' ? plan.price : 0,
                    currency: plan.currency || 'USD',
                    billing_cycle: ['monthly', 'annually', 'one_time'].includes(plan.billingCycle) ? plan.billingCycle : 'monthly',
                    description: plan.description,
                    is_current: true
                })
                .select('id')
                .single();

            if (planError) throw new Error(`Failed to insert plan "${plan.planName}": ${planError.message}`);
            if (!planData) throw new Error(`Insert for plan "${plan.planName}" did not return data.`);
            
            if (plan.features && Array.isArray(plan.features) && plan.features.length > 0) {
                const featuresToInsert = plan.features.map(f => ({
                    plan_id: planData.id,
                    feature_text: f,
                    is_current: true
                }));
                const { error: featureError } = await supabase.from('plan_features').insert(featuresToInsert);
                if (featureError) console.warn(`Could not insert features for plan ${plan.planName}: ${featureError.message}`);
            }
        }
        
        await supabase.from('scrape_sessions').update({ status: 'success', error_message: null }).eq('id', session.id);
        console.log(`âœ… Job for ${competitor.name} completed successfully!`);

    } catch (error: any) {
        console.error(`âŒ Job for ${competitor.name} failed:`, error.message);
        await supabase
            .from('scrape_sessions')
            .update({ status: 'failed', error_message: error.message.substring(0, 500) })
            .eq('id', session.id);
    }
}

// --- Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
function setupListener(): RealtimeChannel {
    console.log('Worker is running and listening for new scrape sessions...');

    const channel = supabase.channel('public:scrape_sessions')
        .on<ScrapeSessionPayload>(
            'postgres_changes',
            { event: 'INSERT', schema: 'public', table: 'scrape_sessions' },
            (payload) => {
                console.log('New scrape session detected!', payload.new);
                if (payload.new.status === 'pending') {
                    // Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… await Ù‡Ù†Ø§ Ù„Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…Ø³ØªÙ…Ø¹ Ø¨Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø³Ø±Ø¹Ø©
                    // ÙˆØ¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
                    processScrapeSession(payload.new);
                }
            }
        )
        .subscribe((status, err) => {
            if (status === 'SUBSCRIBED') console.log('Successfully subscribed to database changes!');
            if (status === 'CHANNEL_ERROR') console.error('Channel error, attempting to reconnect...', err);
            if(status === 'TIMED_OUT') console.warn('Subscription timed out. The connection will be re-established.');
        });

    return channel;
}

const mainChannel = setupListener();

// Ø¥ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ Ù†Ø¸ÙŠÙ
process.on('SIGINT', async () => {
    console.log('Shutting down worker...');
    await supabase.removeChannel(mainChannel);
    process.exit(0);
});